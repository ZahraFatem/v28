<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Guaranteed Sparse Recovery under Linear Transformation | ICML 2013 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Guaranteed Sparse Recovery under Linear Transformation">

  <meta name="citation_author" content="Liu, Ji">

  <meta name="citation_author" content="Yuan, Lei">

  <meta name="citation_author" content="Ye, Jieping">

<meta name="citation_publication_date" content="2013">
<meta name="citation_conference_title" content="Proceedings of The 30th International Conference on Machine Learning">
<meta name="citation_firstpage" content="91">
<meta name="citation_lastpage" content="99">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v28/liu13.pdf">

</head>
<body>

<div id="fixed"> <a align="right" href="http://www.jmlr.org/" target="_top"><img class="jmlr" src="http://jmlr.org/proceedings/papers/img/jmlr.jpg" align="right" border="0"></a> 
<p><br><br>
</p><p align="right"> <a href="http://www.jmlr.org/"> Home Page </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/papers"> Papers
 </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/author-info.html"> Submissions </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/news.html"> 
News </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/scope.html"> 
Scope </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/editorial-board.html"> Editorial Board </a>
 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/announcements.html"> Announcements </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/proceedings"> 
Proceedings </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/mloss">Open 
Source Software</a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/search-jmlr.html"> Search </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/manudb"> Login </a></p>

<br><br>
<p align="right"> <a href="http://jmlr.csail.mit.edu/jmlr.xml"> 
<img src="http://jmlr.org/proceedings/papers/img/RSS.gif" class="rss" alt="RSS Feed">
</a>

</p>
 </div>

<div id="content">

	<h1>Guaranteed Sparse Recovery under Linear Transformation</h1>

	<div id="authors">
	
		Ji Liu,
	
		Lei Yuan,
	
		Jieping Ye
	</div>;
	<div id="info">
		JMLR W&amp;CP 28 
		 (3) 
		: 
		91â€“99, 2013
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We consider the following signal recovery problem: given a measurement matrix <span class="math">\(\Phi\in \mathbb{R}^{n\times p}\)</span> and a noisy observation vector <span class="math">\(c\in \mathbb{R}^{n}\)</span> constructed from <span class="math">\(c =  \Phi\theta^* + \epsilon\)</span> where <span class="math">\(\epsilon\in \mathbb{R}^{n}\)</span> is the noise vector whose entries follow i.i.d. centered sub-Gaussian distribution, how to recover the signal <span class="math">\(\theta^*\)</span> if <span class="math">\(D\theta^*\)</span> is sparse <span>under a linear transformation</span> <span class="math">\(D\in\mathbb{R}^{m\times  p}\)</span>? One natural method using convex optimization is to solve the following problem: <span class="math">\[\min_{\theta}~{1\over 2}\|\Phi\theta - c\|^2 +  \lambda\|D\theta\|_1.\]</span> This paper provides an upper bound of the estimate error and shows the consistency property of this method by assuming that the design matrix <span class="math">\(\Phi\)</span> is a Gaussian random matrix. Specifically, we show 1) in the noiseless case, if the condition number of <span class="math">\(D\)</span> is bounded and the measurement number <span class="math">\(n\geq  \Omega(s\log(p))\)</span> where <span class="math">\(s\)</span> is the sparsity number, then the true solution can be recovered with high probability; and 2) in the noisy case, if the condition number of <span class="math">\(D\)</span> is bounded and the measurement increases faster than <span class="math">\(s\log(p)\)</span>, that is, <span class="math">\(s\log(p)=o(n)\)</span>, the estimate error converges to zero with probability 1 when <span class="math">\(p\)</span> and <span class="math">\(s\)</span> go to infinity. Our results are consistent with those for the special case <span class="math">\(D=\bold{I}_{p\times p}\)</span> (equivalently LASSO) and improve the existing analysis. The condition number of <span class="math">\(D\)</span> plays a critical role in our analysis. We consider the condition numbers in two cases including the fused LASSO and the random graph: the condition number in the fused LASSO case is bounded by a constant, while the condition number in the random graph case is bounded with high probability if <span class="math">\(m\over p\)</span> (i.e., <span class="math">\(\#\text{edge}\over  \#\text{vertex}\)</span>) is larger than a certain constant. Numerical simulations are consistent with our theoretical results.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="liu13.pdf">Download PDF</a></li>
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
