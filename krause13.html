<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Approximation properties of <span>DBNs</span> with binary hidden units and real-valued visible units | ICML 2013 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Approximation properties of {DBNs} with binary hidden units and real-valued visible units">

  <meta name="citation_author" content="Krause, Oswin">

  <meta name="citation_author" content="Fischer, Asja">

  <meta name="citation_author" content="Glasmachers, Tobias">

  <meta name="citation_author" content="Igel, Christian">

<meta name="citation_publication_date" content="2013">
<meta name="citation_conference_title" content="Proceedings of The 30th International Conference on Machine Learning">
<meta name="citation_firstpage" content="419">
<meta name="citation_lastpage" content="426">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v28/krause13.pdf">

</head>
<body>

<div id="fixed"> <a align="right" href="http://www.jmlr.org/" target="_top"><img class="jmlr" src="http://jmlr.org/proceedings/papers/img/jmlr.jpg" align="right" border="0"></a> 
<p><br><br>
</p><p align="right"> <a href="http://www.jmlr.org/"> Home Page </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/papers"> Papers
 </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/author-info.html"> Submissions </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/news.html"> 
News </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/scope.html"> 
Scope </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/editorial-board.html"> Editorial Board </a>
 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/announcements.html"> Announcements </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/proceedings"> 
Proceedings </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/mloss">Open 
Source Software</a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/search-jmlr.html"> Search </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/manudb"> Login </a></p>

<br><br>
<p align="right"> <a href="http://jmlr.csail.mit.edu/jmlr.xml"> 
<img src="http://jmlr.org/proceedings/papers/img/RSS.gif" class="rss" alt="RSS Feed">
</a>

</p>
 </div>

<div id="content">

	<h1>Approximation properties of <span>DBNs</span> with binary hidden units and real-valued visible units</h1>

	<div id="authors">
	
		Oswin Krause,
	
		Asja Fischer,
	
		Tobias Glasmachers,
	
		Christian Igel
	</div>;
	<div id="info">
		JMLR W&amp;CP 28 
		 (1) 
		: 
		419â€“426, 2013
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Deep belief networks (DBNs) can approximate any distribution over fixed-length binary vectors. However, DBNs are frequently applied to model real-valued data, and so far little is known about their representational power in this case. We analyze the approximation properties of DBNs with two layers of binary hidden units and visible units with conditional distributions from the exponential family. It is shown that these DBNs can, under mild assumptions, model any additive mixture of distributions from the exponential family with independent variables. An arbitrarily good approximation in terms of Kullback-Leibler divergence of an m-dimensional mixture distribution with n components can be achieved by a DBN with m visible variables and n and n+1 hidden variables in the first and second hidden layer, respectively. Furthermore, relevant infinite mixtures can be approximated arbitrarily well by a DBN with a finite number of neurons. This includes the important special case of an infinite mixture of Gaussian distributions with fixed variance restricted to a compact domain, which in turn can approximate any strictly positive density over this domain.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="krause13.pdf">Download PDF</a></li>
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
