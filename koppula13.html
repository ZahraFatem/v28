<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation | ICML 2013 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation">

  <meta name="citation_author" content="Koppula, Hema">

  <meta name="citation_author" content="Saxena, Ashutosh">

<meta name="citation_publication_date" content="2013">
<meta name="citation_conference_title" content="Proceedings of The 30th International Conference on Machine Learning">
<meta name="citation_firstpage" content="792">
<meta name="citation_lastpage" content="800">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v28/koppula13.pdf">

</head>
<body>

<div id="fixed"> <a align="right" href="http://www.jmlr.org/" target="_top"><img class="jmlr" src="http://jmlr.org/proceedings/papers/img/jmlr.jpg" align="right" border="0"></a> 
<p><br><br>
</p><p align="right"> <a href="http://www.jmlr.org/"> Home Page </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/papers"> Papers
 </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/author-info.html"> Submissions </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/news.html"> 
News </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/scope.html"> 
Scope </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/editorial-board.html"> Editorial Board </a>
 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/announcements.html"> Announcements </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/proceedings"> 
Proceedings </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/mloss">Open 
Source Software</a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/search-jmlr.html"> Search </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/manudb"> Login </a></p>

<br><br>
<p align="right"> <a href="http://jmlr.csail.mit.edu/jmlr.xml"> 
<img src="http://jmlr.org/proceedings/papers/img/RSS.gif" class="rss" alt="RSS Feed">
</a>

</p>
 </div>

<div id="content">

	<h1>Learning Spatio-Temporal Structure from RGB-D Videos for Human Activity Detection and Anticipation</h1>

	<div id="authors">
	
		Hema Koppula,
	
		Ashutosh Saxena
	</div>;
	<div id="info">
		JMLR W&amp;CP 28 
		 (3) 
		: 
		792â€“800, 2013
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We consider the problem of detecting past activities as well as anticipating which activity will happen in the future and how. We start by modeling the rich spatio-temporal relations between human poses and objects (called affordances) using a conditional random field (CRF). However, because of the ambiguity in the temporal segmentation of the sub-activities that constitute an activity, in the past as well as in the future, multiple graph structures are possible. In this paper, we reason about these alternate possibilities by reasoning over multiple possible graph structures. We obtain them by approximating the graph with only additive features, which lends to efficient dynamic programming. Starting with this proposal graph structure, we then design moves to obtain several other likely graph structures. We then show that our approach improves the state-of-the-art significantly for detecting past activities as well as for anticipating future activities, on a dataset of 120 activity videos collected from four subjects.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="koppula13.pdf">Download PDF</a></li>
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
