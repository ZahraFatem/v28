---
pdf: http://proceedings.mlr.press/v28/chen13h.pdf
supplementary: Supplementary:chen13h-supp.pdf
number: 3
section: cycle-3
title: Robust Sparse Regression under Adversarial Corruption
abstract: 'We consider high dimensional sparse regression with arbitrary – possibly,
  severe or coordinated – errors in the covariates matrix. We are interested in understanding
  how many corruptions we can tolerate, while identifying the correct support. To
  the best of our knowledge, neither standard outlier rejection techniques, nor recently
  developed robust regression algorithms (that focus only on corrupted response variables),
  nor recent algorithms for dealing with stochastic noise or erasures, can provide
  guarantees on support recovery. As we show, neither can the natural brute force
  algorithm that takes exponential time to find the subset of data and support columns,
  that yields the smallest regression error.     We explore the power of a simple
  idea: replace the essential linear algebraic calculation – the inner product – with
  a robust counterpart that cannot be greatly affected by a controlled number of arbitrarily
  corrupted points: the trimmed inner product. We consider three popular algorithms
  in the uncorrupted setting: Thresholding Regression, Lasso, and the Dantzig selector,
  and show that the counterparts obtained using the trimmed inner product are provably
  robust.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: chen13h
month: 0
tex_title: Robust Sparse Regression under Adversarial Corruption
firstpage: 774
lastpage: 782
page: 774-782
order: 774
cycles: false
author:
- given: Yudong
  family: Chen
- given: Constantine
  family: Caramanis
- given: Shie
  family: Mannor
date: 2013-05-26
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 5
  - 26
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
