---
pdf: http://proceedings.mlr.press/v28/wang13a.pdf
number: 2
section: cycle-2
title: Fast dropout training
abstract: 'Preventing feature co-adaptation by encouraging independent contributions
  from different features often improves classification and regression performance.  Dropout
  training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden
  units and input features during training of neural networks. However, repeatedly
  sampling a random subset of input features makes training much slower. Based on
  an examination of the implied objective function of dropout training, we show how
  to do fast dropout training by sampling from or integrating a Gaussian approximation,
  instead of doing Monte Carlo optimization of this objective.  This approximation,
  justified by the central limit theorem and empirical evidence, gives an order of
  magnitude speedup and more stability.  We show how to do fast dropout training for
  classification, regression, and multilayer neural networks. Beyond dropout, our
  technique is extended to integrate out other types of noise and small image transformations. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: wang13a
month: 0
tex_title: Fast dropout training
firstpage: 118
lastpage: 126
page: 118-126
cycles: false
author:
- given: Sida
  family: Wang
- given: Christopher
  family: Manning
date: 2013-05-13
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 5
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
