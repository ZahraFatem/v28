---
pdf: http://proceedings.mlr.press/v28/coates13.pdf
number: 2
section: cycle-3
title: Deep learning with COTS HPC systems
abstract: 'Scaling up deep learning algorithms has been shown to lead to increased
  performance in benchmark tasks and to enable discovery of complex high-level features.  Recent
  efforts to train extremely large networks (with over 1 billion parameters) have
  relied on cloud-like computing infrastructure and thousands of CPU cores.  In this
  paper, we present technical details and results from our own system based on Commodity
  Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU
  servers with Infiniband interconnects and MPI.  Our system is able to train 1 billion
  parameter networks on just 3 machines in a couple of days, and we show that it can
  scale to networks with over 11 billion parameters using just 16 machines.  As this
  infrastructure is much more easily marshaled by others, the approach enables much
  wider-spread research with extremely large neural networks.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: coates13
month: 0
tex_title: Deep learning with COTS HPC systems
firstpage: 1337
lastpage: 1345
page: 1337-1345
cycles: false
author:
- given: Adam
  family: Coates
- given: Brody
  family: Huval
- given: Tao
  family: Wang
- given: David
  family: Wu
- given: Bryan
  family: Catanzaro
- given: Ng
  family: Andrew
date: 2013-05-26
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 5
  - 26
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
