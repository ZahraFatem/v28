---
pdf: http://proceedings.mlr.press/v28/ranganath13.pdf
supplementary: Supplementary:ranganath13-supp.pdf
number: '2'
section: cycle-2
title: An Adaptive Learning Rate for Stochastic Variational Inference
abstract: Stochastic variational inference finds good posterior approximations of
  probabilistic models with very large data sets.  It optimizes the variational objective
  with stochastic optimization, following noisy estimates of the natural gradient.  Operationally,
  stochastic inference iteratively subsamples from the data, analyzes the subsample,
  and updates parameters with a decreasing learning rate. However, the algorithm is
  sensitive to that rate, which usually requires hand-tuning to each application.
  We solve this problem by developing an adaptive learning rate for stochastic inference.  Our
  method requires no tuning and is easily implemented with computations already made
  in the algorithm.  We demonstrate our approach with latent Dirichlet allocation
  applied to three large text corpora.  Inference with the adaptive learning rate
  converges faster and to a better approximation than the best settings of hand-tuned
  rates.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: ranganath13
month: 0
tex_title: An Adaptive Learning Rate for Stochastic Variational Inference
firstpage: 298
lastpage: 306
page: 298-306
sections: 
author:
- given: Rajesh
  family: Ranganath
- given: Chong
  family: Wang
- given: Blei
  family: David
- given: Eric
  family: Xing
date: 2013-02-13
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 2
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
