---
pdf: http://proceedings.mlr.press/v28/grinberg13/grinberg13.pdf
supplementary: Supplementary:grinberg13-supp.pdf
title: Average Reward Optimization Objective In Partially Observable Domains
number: '1'
section: cycle-1
abstract: We consider the problem of average reward optimization in domains with partial
  observability, within the modeling framework of linear predictive state representations
  (PSRs). The key to average-reward computation is to have a well-defined stationary
  behavior of a system, so the required averages can be computed. If, additionally,
  the stationary behavior varies smoothly with changes in policy parameters, average-reward
  control through policy search also becomes a possibility. In this paper, we show
  that PSRs have a well-behaved stationary distribution, which is a rational function
  of policy parameters.  Based on this result, we define a related reward process
  particularly suitable for average reward optimization, and analyze its properties.
  We show that in such a predictive state reward process, the average reward is a
  rational function of the policy parameters, whose complexity depends on the dimension
  of the underlying linear PSR. This result suggests that average reward-based policy
  search methods can be effective when the dimension of the system is small, even
  when the system representation in the POMDP framework requires many hidden states.
  We provide illustrative examples of this type.
layout: inproceedings
id: grinberg13
month: 0
firstpage: 320
lastpage: 328
page: 320-328
sections: 
author:
- given: Yuri
  family: Grinberg
- given: Doina
  family: Precup
date: 2013-02-13
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 2
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
