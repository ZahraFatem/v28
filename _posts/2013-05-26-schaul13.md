---
pdf: http://proceedings.mlr.press/v28/schaul13.pdf
supplementary: http://proceedings.mlr.press/v28/schaul13-supp.pdf
number: 3
section: cycle-3
title: No more pesky learning rates
abstract: The performance of stochastic gradient descent (SGD) depends critically
  on how learning rates are tuned and decreased over time. We propose a method to
  automatically adjust multiple learning rates so as to minimize the expected error
  at any one time. The method relies on local gradient variations across samples.
  In our approach, learning rates can increase as well as decrease, making it suitable
  for non-stationary problems. Using a number of convex and non-convex learning tasks,
  we show that the resulting algorithm matches the performance of the best settings
  obtained through systematic search, and effectively removes the need for learning
  rate tuning.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: schaul13
month: 0
tex_title: No more pesky learning rates
firstpage: 343
lastpage: 351
page: 343-351
order: 343
cycles: false
author:
- given: Tom
  family: Schaul
- given: Sixin
  family: Zhang
- given: Yann
  family: LeCun
date: 2013-05-26
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 5
  - 26
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
