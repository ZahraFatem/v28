---
pdf: http://proceedings.mlr.press/v28/gittens13.pdf
number: 3
section: cycle-3
title: Revisiting the Nystrom method for improved large-scale machine learning
abstract: We reconsider randomized algorithms for the low-rank approximation of SPSD
  matrices such as Laplacian and kernel matrices that arise in data analysis and machine
  learning applications.    Our main results consist of an empirical evaluation of
  the performance quality and running time of sampling and projection methods on a
  diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling
  versus projection methods, and they point to differences between uniform and nonuniform
  sampling methods based on leverage scores.    We complement our empirical results
  with a suite of worst-case theoretical bounds for both random sampling and random
  projection methods. These bounds are qualitatively superior to existing boundsâ€”
  e.g., improved additive-error bounds for spectral and Frobenius norm error and relative-error
  bounds for trace norm error.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: gittens13
month: 0
tex_title: Revisiting the Nystrom method for improved large-scale machine learning
firstpage: 567
lastpage: 575
page: 567-575
order: 567
cycles: false
author:
- given: Alex
  family: Gittens
- given: Michael
  family: Mahoney
date: 2013-05-26
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 5
  - 26
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
