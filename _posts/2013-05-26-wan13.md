---
pdf: http://proceedings.mlr.press/v28/wan13.pdf
supplementary: Supplementary:wan13-supp.pdf
number: 2
section: cycle-3
title: Regularization of Neural Networks using DropConnect
abstract: We introduce DropConnect, a generalization of DropOut, for regularizing
  large fully-connected layers within neural networks. When training with Dropout,
  a randomly selected subset of activations are set to zero within each layer. DropConnect
  instead sets a randomly selected subset of weights within the network to zero. Each
  unit thus receives input from a random subset of units in the previous layer. We
  derive a bound on the generalization performance of both Dropout and DropConnect.
  We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show
  state-of-the-art results on several image recoginition benchmarks can be obtained
  by aggregating multiple DropConnect-trained models.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: wan13
month: 0
tex_title: Regularization of Neural Networks using DropConnect
firstpage: 1058
lastpage: 1066
page: 1058-1066
cycles: false
author:
- given: Li
  family: Wan
- given: Matthew
  family: Zeiler
- given: Sixin
  family: Zhang
- given: Yann
  family: Le Cun
- given: Rob
  family: Fergus
date: 2013-05-26
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 5
  - 26
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
