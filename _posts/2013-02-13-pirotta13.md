---
pdf: http://proceedings.mlr.press/v28/pirotta13/pirotta13.pdf
supplementary: Supplementary:pirotta13-supp.pdf
number: '3'
section: cycle-3
title: Safe Policy Iteration
abstract: This paper presents a study of the policy improvement step that can be usefully
  exploited by approximate policy-iteration algorithms.  When either the policy evaluation
  step or the policy improvement step returns an approximated result, the sequence
  of policies produced by policy iteration may not be monotonically increasing, and
  oscillations may occur.  To address this issue, we consider safe policy improvements,
  i.e., at each iteration we search for a policy that maximizes a lower bound to the
  policy improvement w.r.t. the current policy. When no improving policy can be found
  the algorithm stops.  We propose two safe policy-iteration algorithms that differ
  in the way the next policy is chosen w.r.t. the estimated greedy policy.  Besides
  being theoretically derived and discussed, the proposed algorithms are empirically
  evaluated and compared with state-of-the-art approaches on some chain-walk domains
  and on the Blackjack card game.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: pirotta13
month: 0
firstpage: 307
lastpage: 315
page: 307-315
sections: 
author:
- given: Matteo
  family: Pirotta
- given: Marcello
  family: Restelli
- given: Alessio
  family: Pecorino
- given: Daniele
  family: Calandriello
date: 2013-02-13
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 2
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
