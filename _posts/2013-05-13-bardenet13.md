---
pdf: http://proceedings.mlr.press/v28/bardenet13.pdf
supplementary: Supplementary:bardenet13-supp.pdf
number: 2
section: cycle-2
title: Collaborative hyperparameter tuning
abstract: Hyperparameter learning has traditionally been a manual task because of
  the limited number of trials. Today’s computing infrastructures allow bigger evaluation
  budgets, thus opening the way for algorithmic approaches. Recently, surrogate-based
  optimization was successfully applied to hyperparameter learning for deep belief
  networks and to WEKA classifiers. The methods combined brute force computational
  power with model building about the behavior of the error function in the hyperparameter
  space, and they could significantly improve on manual hyperparameter tuning. What
  may make experienced practitioners even better at hyperparameter optimization is
  their ability to generalize across similar learning problems. In this paper, we
  propose a generic method to incorporate knowledge from previous experiments when
  simultaneously tuning a learning algorithm on new problems at hand. To this end,
  we combine surrogate-based ranking and optimization techniques for surrogate-based
  collaborative tuning (SCoT). We demonstrate SCoT in two experiments where it outperforms
  standard tuning techniques and single-problem surrogate-based optimization.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: bardenet13
month: 0
tex_title: Collaborative hyperparameter tuning
firstpage: 199
lastpage: 207
page: 199-207
cycles: false
author:
- given: "R\x1Aémi"
  family: Bardenet
- given: "Má\x1Aty\x1Aás"
  family: Brendel
- given: Balázs
  family: Kégl
- given: Michèle
  family: Sebag
date: 2013-05-13
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 5
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
