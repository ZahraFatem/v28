---
pdf: http://proceedings.mlr.press/v28/shamir13.pdf
title: 'Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results
  and Optimal Averaging Schemes'
number: '1'
section: cycle-1
abstract: Stochastic Gradient Descent (SGD) is one of the simplest and most popular
  stochastic optimization methods. While it has already been theoretically studied
  for decades, the classical analysis usually required non-trivial smoothness assumptions,
  which do not apply to many modern applications of SGD with non-smooth objective
  functions such as support vector machines.  In this paper, we investigate the performance
  of SGD \emphwithout such smoothness assumptions, as well as a running average scheme
  to convert the SGD iterates to a solution with optimal optimization accuracy. In
  this framework, we prove that after T rounds, the suboptimality of the \emphlast
  SGD iterate scales as O(\log(T)/\sqrtT) for non-smooth convex objective functions,
  and O(\log(T)/T) in the non-smooth strongly convex case. To the best of our knowledge,
  these are the first bounds of this kind, and almost match the minimax-optimal rates
  obtainable by appropriate averaging schemes. We also propose a new and simple averaging
  scheme, which not only attains optimal rates, but can also be easily computed on-the-fly
  (in contrast, the suffix averaging scheme proposed in \citetRakhShaSri12arxiv is
  not as simple to implement). Finally, we provide some experimental illustrations.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: shamir13
month: 0
firstpage: 71
lastpage: 79
page: 71-79
sections: 
author:
- given: Ohad
  family: Shamir
- given: Tong
  family: Zhang
date: 2013-02-13
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 2
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
