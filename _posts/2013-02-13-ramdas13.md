---
pdf: http://proceedings.mlr.press/v28/ramdas13/ramdas13.pdf
supplementary: Supplementary:ramdas13-supp.pdf
title: Optimal rates for stochastic convex optimization under Tsybakov noise condition
number: '1'
section: cycle-1
abstract: We focus on the problem of minimizing a convex function f over a convex
  set S given T queries to a stochastic first order oracle. We argue that the complexity
  of convex minimization is only determined by the rate of growth of the function
  around its minimum x^*_f,S, as quantified by a Tsybakov-like noise condition. Specifically,
  we prove that if f grows at least as fast as \|x-x^*_f,S\|^κaround its minimum,
  for some κ> 1, then the optimal rate of learning f(x^*_f,S) is  Θ(T^-\fracκ2κ-2).
  The classic rate Θ(1/\sqrt T) for convex functions and Θ(1/T) for strongly convex
  functions are special cases of our result for κ→∞and κ=2, and even faster rates
  are attained for 1 < κ< 2. We also derive tight bounds for the complexity of learning
  x_f,S^*, where the optimal rate is Θ(T^-\frac12κ-2). Interestingly, these precise
  rates also characterize the complexity of active learning and our results further
  strengthen the connections between the fields of active learning and convex optimization,
  both of which rely on feedback-driven queries.
layout: inproceedings
id: ramdas13
month: 0
firstpage: 365
lastpage: 373
page: 365-373
sections: 
author:
- given: Aaditya
  family: Ramdas
- given: Aarti
  family: Singh
date: 2013-02-13
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 2
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
