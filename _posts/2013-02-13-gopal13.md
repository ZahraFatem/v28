---
pdf: http://proceedings.mlr.press/v28/gopal13.pdf
number: '2'
section: cycle-2
title: Distributed training of Large-scale Logistic models
abstract: Regularized Multinomial Logistic regression has emerged as one of the most
  common methods for performing data classification and analysis. With the advent
  of large-scale data it is common to find scenarios where the number of possible
  multinomial outcomes is large (in the order of thousands to tens of thousands).
  In such cases, the computational cost of training logistic models or even simply
  iterating through all the model parameters is prohibitively expensive. In this paper,
  we propose a training method for large-scale multinomial logistic models that breaks
  this bottleneck by enabling parallel optimization of the likelihood objective. Our
  experiments on large-scale datasets showed an order of magnitude reduction in training
  time.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: gopal13
month: 0
tex_title: Distributed training of Large-scale Logistic models
firstpage: 289
lastpage: 297
page: 289-297
sections: 
author:
- given: Siddharth
  family: Gopal
- given: Yiming
  family: Yang
date: 2013-02-13
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 2
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
