---
pdf: http://proceedings.mlr.press/v28/suzuki13.pdf
supplementary: Supplementary:suzuki13-supp.pdf
title: Dual Averaging and Proximal Gradient Descent for Online Alternating Direction
  Multiplier Method
number: '1'
section: cycle-1
abstract: We develop new stochastic optimization methods that are applicable to   a
  wide range of structured regularizations.  Basically our methods are combinations
  of   basic stochastic optimization techniques and Alternating Direction Multiplier
  Method (ADMM).  ADMM is a general framework for optimizing a composite function,  and
  has a wide range of applications.  We propose two types of online variants of ADMM,   which
  correspond to online proximal gradient descent and regularized dual averaging respectively.  The
  proposed algorithms are computationally efficient and easy to implement.  Our methods
  yield O(1/\sqrtT) convergence of the expected risk.  Moreover, the online proximal
  gradient descent type method yields   O(\log(T)/T) convergence for a strongly convex
  loss.  Numerical experiments show effectiveness of our methods in learning tasks
  with structured sparsity  such as overlapped group lasso.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: suzuki13
month: 0
firstpage: 392
lastpage: 400
page: 392-400
sections: 
author:
- given: Taiji
  family: Suzuki
date: 2013-02-13
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 2
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
