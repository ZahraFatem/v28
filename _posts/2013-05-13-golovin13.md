---
pdf: http://proceedings.mlr.press/v28/golovin13.pdf
number: 2
section: cycle-2
title: Large-Scale Learning with Less RAM via Randomization
abstract: We reduce the memory footprint of popular large-scale online learning methods
  by projecting our weight vector onto a coarse discrete set using randomized rounding.
  Compared to standard 32-bit float encodings, this reduces RAM usage by more than
  50% during training and by up 95% when making predictions from a fixed model, with
  almost no loss in accuracy. We also show that randomized counting can be used to
  implement per-coordinate learning rates, improving model quality with little additional
  RAM. We prove these memory-saving methods achieve regret guarantees similar to their
  exact variants. Empirical evaluation confirms excellent performance, dominating
  standard approaches across memory versus accuracy tradeoffs.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: golovin13
month: 0
tex_title: Large-Scale Learning with Less RAM via Randomization
firstpage: 325
lastpage: 333
page: 325-333
order: 325
cycles: false
author:
- given: Daniel
  family: Golovin
- given: D.
  family: Sculley
- given: Brendan
  family: McMahan
- given: Michael
  family: Young
date: 2013-05-13
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 5
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
