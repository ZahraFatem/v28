---
pdf: http://proceedings.mlr.press/v28/hennig13.pdf
title: Fast Probabilistic Optimization from Noisy Gradients
number: '1'
section: cycle-1
abstract: 'Stochastic gradient descent remains popular in large-scale machine learning,
  on account of its very low computational cost and robustness to noise. However,
  gradient descent is only linearly efficient and not transformation invariant. Scaling
  by a local measure can substantially improve its performance. One natural choice
  of such a scale is the Hessian of the objective function: Were it available, it
  would turn linearly efficient gradient descent into the quadratically efficient
  Newton-Raphson optimization. Existing covariant methods, though, are either super-linearly
  expensive or do not address noise. Generalising recent results, this paper constructs
  a nonparametric Bayesian quasi-Newton algorithm that learns gradient and Hessian
  from noisy evaluations of the gradient. Importantly, the resulting algorithm, like
  stochastic gradient descent, has cost linear in the number of input dimensions.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: hennig13
month: 0
firstpage: 62
lastpage: 70
page: 62-70
sections: 
author:
- given: Philipp
  family: Hennig
date: 2013-02-13
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 2
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
