---
pdf: http://proceedings.mlr.press/v28/pascanu13.pdf
supplementary: http://proceedings.mlr.press/v28/pascanu13-supp.pdf
number: 3
section: cycle-3
title: On the difficulty of training recurrent neural networks
abstract: 'There are two widely known issues with properly training recurrent neural
  networks, the vanishing and the exploding gradient problems detailed in Bengio et
  al. (1994). In this paper we attempt to improve the understanding of the underlying
  issues by exploring these problems from an analytical, a geometric and a dynamical
  systems perspective. Our analysis is used to justify a simple yet effective solution.
  We propose a gradient norm clipping strategy to deal with exploding gradients and
  a soft constraint for the vanishing gradients problem. We validate empirically our
  hypothesis and proposed solutions in the experimental section.  '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: pascanu13
month: 0
tex_title: On the difficulty of training recurrent neural networks
firstpage: 1310
lastpage: 1318
page: 1310-1318
order: 1310
cycles: false
author:
- given: Razvan
  family: Pascanu
- given: Tomas
  family: Mikolov
- given: Yoshua
  family: Bengio
date: 2013-05-26
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 5
  - 26
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
