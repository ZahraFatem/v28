---
pdf: http://proceedings.mlr.press/v28/vandermaaten13/vandermaaten13.pdf
title: Learning with Marginalized Corrupted Features
number: '1'
section: cycle-1
abstract: 'The goal of machine learning is to develop predictors that generalize well
  to test data. Ideally, this is achieved by training on very large (infinite) training
  data sets that capture all variations in the data distribution. In the case of finite
  training data, an effective solution is to extend the training set with artificially
  created examples – which, however, is also computationally costly. We propose to
  corrupt training examples with noise from known distributions within the exponential
  family and present a novel learning algorithm, called marginalized corrupted features
  (MCF), that trains robust predictors by minimizing the expected value of the loss
  function under the corrupting distribution – essentially learning with infinitely
  many (corrupted) training examples. We show empirically on a variety of data sets
  that MCF classifiers can be trained efficiently, may generalize substantially better
  to test data, and are more robust to feature deletion at test time.  '
layout: inproceedings
id: vandermaaten13
month: 0
firstpage: 410
lastpage: 418
page: 410-418
sections: 
author:
- given: Laurens
  family: Maaten
- given: Minmin
  family: Chen
- given: Stephen
  family: Tyree
- given: Kilian
  family: Weinberger
date: 2013-02-13
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 2
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
