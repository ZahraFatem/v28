---
pdf: http://proceedings.mlr.press/v28/kuzborskij13.pdf
number: '3'
section: cycle-3
title: Stability and Hypothesis Transfer Learning
abstract: We consider the transfer learning scenario, where the learner does not have
  access to the source domain directly, but rather operates on the basis of hypotheses
  induced from it â€“ the Hypothesis Transfer Learning (HTL) problem. Particularly,
  we conduct a theoretical analysis of HTL by considering the algorithmic stability
  of a class of HTL algorithms based on Regularized Least Squares with biased regularization.
  We show that the relatedness of source and target domains accelerates the convergence
  of the Leave-One-Out error to the generalization error, thus enabling the use of
  the Leave-One-Out error to find the optimal transfer parameters, even in the presence
  of a small training set. In case of unrelated domains we also suggest a theoretically
  principled way to prevent negative transfer, so that in the limit we recover the
  performance of the algorithm not using any knowledge from the source domain.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: kuzborskij13
month: 0
tex_title: Stability and Hypothesis Transfer Learning
firstpage: 942
lastpage: 950
page: 942-950
sections: 
author:
- given: Ilja
  family: Kuzborskij
- given: Francesco
  family: Orabona
date: 2013-02-13
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 2
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
