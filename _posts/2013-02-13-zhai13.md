---
pdf: http://proceedings.mlr.press/v28/zhai13.pdf
title: Online Latent {D}irichlet Allocation with Infinite Vocabulary
number: '1'
section: cycle-1
abstract: Topic models based on latent Dirichlet allocation (LDA) assume a predefined
  vocabulary a priori. This is reasonable in batch settings, but it is not reasonable
  when data are revealed over time, as is the case with streaming / online algorithms.
  To address this lacuna, we extend LDA by drawing topics from a Dirichlet process
  whose base distribution is a distribution over all strings rather than from a finite
  Dirichlet. We develop inference using online variational inference and because we
  only can consider a finite number of words for each truncated topic propose heuristics
  to dynamically organize, expand, and contract the set of words we consider in our
  vocabulary truncation. We show our model can successfully incorporate new words
  as it encounters new terms and that it performs better than online LDA in evaluations
  of topic quality and classification performance.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhai13
month: 0
firstpage: 561
lastpage: 569
page: 561-569
sections: 
author:
- given: Ke
  family: Zhai
- given: Jordan
  family: Boyd-Graber
date: 2013-02-13
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 2
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
