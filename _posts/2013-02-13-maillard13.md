---
pdf: http://proceedings.mlr.press/v28/maillard13.pdf
title: Optimal Regret Bounds for  Selecting the State Representation in Reinforcement
  Learning
number: '1'
section: cycle-1
abstract: We consider an agent interacting with an environment in a single stream
  of actions, observations, and rewards, with no reset. This process is not assumed
  to be a Markov Decision Process (MDP). Rather, the agent has several representations
  (mapping histories of past interactions to a discrete state space) of the environment
  with unknown dynamics, only some of which result in an MDP. The goal is to minimize
  the average regret criterion against an agent who knows   an MDP representation
  giving the highest optimal reward, and acts optimally in it. Recent regret bounds
  for this setting are of order O(T^2/3) with an additive term constant yet exponential
  in some characteristics of the optimal MDP.  We propose an algorithm whose regret
  after T time steps is O(\sqrtT),  with all constants reasonably small.  This is
  optimal in T since O(\sqrtT) is the optimal regret in the setting of learning in
  a (single discrete) MDP.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: maillard13
month: 0
firstpage: 543
lastpage: 551
page: 543-551
sections: 
author:
- given: Odalric-Ambrym
  family: Maillard
- given: Phuong
  family: Nguyen
- given: Ronald
  family: Ortner
- given: Daniil
  family: Ryabko
date: 2013-02-13
address: Atlanta, Georgia, USA
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Machine Learning
volume: '28'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 2
  - 13
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
