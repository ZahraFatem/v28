<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Optimal Regret Bounds for Selecting the State Representation in Reinforcement Learning | ICML 2013 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Optimal Regret Bounds for  Selecting the State Representation in Reinforcement Learning">

  <meta name="citation_author" content="Maillard, Odalric-Ambrym">

  <meta name="citation_author" content="Nguyen, Phuong">

  <meta name="citation_author" content="Ortner, Ronald">

  <meta name="citation_author" content="Ryabko, Daniil">

<meta name="citation_publication_date" content="2013">
<meta name="citation_conference_title" content="Proceedings of The 30th International Conference on Machine Learning">
<meta name="citation_firstpage" content="543">
<meta name="citation_lastpage" content="551">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v28/maillard13.pdf">

</head>
<body>

<div id="fixed"> <a align="right" href="http://www.jmlr.org/" target="_top"><img class="jmlr" src="http://jmlr.org/proceedings/papers/img/jmlr.jpg" align="right" border="0"></a> 
<p><br><br>
</p><p align="right"> <a href="http://www.jmlr.org/"> Home Page </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/papers"> Papers
 </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/author-info.html"> Submissions </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/news.html"> 
News </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/scope.html"> 
Scope </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/editorial-board.html"> Editorial Board </a>
 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/announcements.html"> Announcements </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/proceedings"> 
Proceedings </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/mloss">Open 
Source Software</a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/search-jmlr.html"> Search </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/manudb"> Login </a></p>

<br><br>
<p align="right"> <a href="http://jmlr.csail.mit.edu/jmlr.xml"> 
<img src="http://jmlr.org/proceedings/papers/img/RSS.gif" class="rss" alt="RSS Feed">
</a>

</p>
 </div>

<div id="content">

	<h1>Optimal Regret Bounds for Selecting the State Representation in Reinforcement Learning</h1>

	<div id="authors">
	
		Odalric-Ambrym Maillard,
	
		Phuong Nguyen,
	
		Ronald Ortner,
	
		Daniil Ryabko
	</div>;
	<div id="info">
		JMLR W&amp;CP 28 
		 (1) 
		: 
		543â€“551, 2013
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We consider an agent interacting with an environment in a single stream of actions, observations, and rewards, with no reset. This process is not assumed to be a Markov Decision Process (MDP). Rather, the agent has several representations (mapping histories of past interactions to a discrete state space) of the environment with unknown dynamics, only some of which result in an MDP. The goal is to minimize the average regret criterion against an agent who knows an MDP representation giving the highest optimal reward, and acts optimally in it. Recent regret bounds for this setting are of order <span class="math">\(O(T^{2/3})\)</span> with an additive term constant yet exponential in some characteristics of the optimal MDP. We propose an algorithm whose regret after T time steps is <span class="math">\(O(\sqrt{T})\)</span>, with all constants reasonably small. This is optimal in T since <span class="math">\(O(\sqrt{T})\)</span> is the optimal regret in the setting of learning in a (single discrete) MDP.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="maillard13.pdf">Download PDF</a></li>
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
