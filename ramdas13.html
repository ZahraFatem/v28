<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Optimal rates for stochastic convex optimization under Tsybakov noise condition | ICML 2013 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Optimal rates for stochastic convex optimization under Tsybakov noise condition">

  <meta name="citation_author" content="Ramdas, Aaditya">

  <meta name="citation_author" content="Singh, Aarti">

<meta name="citation_publication_date" content="2013">
<meta name="citation_conference_title" content="Proceedings of The 30th International Conference on Machine Learning">
<meta name="citation_firstpage" content="365">
<meta name="citation_lastpage" content="373">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v28/ramdas13.pdf">

</head>
<body>

<div id="fixed"> <a align="right" href="http://www.jmlr.org/" target="_top"><img class="jmlr" src="http://jmlr.org/proceedings/papers/img/jmlr.jpg" align="right" border="0"></a> 
<p><br><br>
</p><p align="right"> <a href="http://www.jmlr.org/"> Home Page </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/papers"> Papers
 </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/author-info.html"> Submissions </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/news.html"> 
News </a> 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/scope.html"> 
Scope </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/editorial-board.html"> Editorial Board </a>
 

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/announcements.html"> Announcements </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/proceedings"> 
Proceedings </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/mloss">Open 
Source Software</a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/search-jmlr.html"> Search </a>

</p><p align="right"> <a href="http://jmlr.csail.mit.edu/manudb"> Login </a></p>

<br><br>
<p align="right"> <a href="http://jmlr.csail.mit.edu/jmlr.xml"> 
<img src="http://jmlr.org/proceedings/papers/img/RSS.gif" class="rss" alt="RSS Feed">
</a>

</p>
 </div>

<div id="content">

	<h1>Optimal rates for stochastic convex optimization under Tsybakov noise condition</h1>

	<div id="authors">
	
		Aaditya Ramdas,
	
		Aarti Singh
	</div>;
	<div id="info">
		JMLR W&amp;CP 28 
		 (1) 
		: 
		365â€“373, 2013
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We focus on the problem of minimizing a convex function <span class="math">\(f\)</span> over a convex set <span class="math">\(S\)</span> given <span class="math">\(T\)</span> queries to a stochastic first order oracle. We argue that the complexity of convex minimization is only determined by the rate of growth of the function around its minimum <span class="math">\(x^*_{f,S}\)</span>, as quantified by a Tsybakov-like noise condition. Specifically, we prove that if <span class="math">\(f\)</span> grows at least as fast as <span class="math">\(\|x-x^*_{f,S}\|^\kappa\)</span> around its minimum, for some <span class="math">\(\kappa &gt; 1\)</span>, then the optimal rate of learning <span class="math">\(f(x^*_{f,S})\)</span> is <span class="math">\(\Theta(T^{-\frac{\kappa}{2\kappa-2}})\)</span>. The classic rate <span class="math">\(\Theta(1/\sqrt T)\)</span> for convex functions and <span class="math">\(\Theta(1/T)\)</span> for strongly convex functions are special cases of our result for <span class="math">\(\kappa \rightarrow \infty\)</span> and <span class="math">\(\kappa=2\)</span>, and even faster rates are attained for <span class="math">\(1 &lt; \kappa &lt; 2\)</span>. We also derive tight bounds for the complexity of learning <span class="math">\(x_{f,S}^*\)</span>, where the optimal rate is <span class="math">\(\Theta(T^{-\frac{1}{2\kappa-2}})\)</span>. Interestingly, these precise rates also characterize the complexity of active learning and our results further strengthen the connections between the fields of active learning and convex optimization, both of which rely on feedback-driven queries.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="ramdas13.pdf">Download PDF</a></li>
			
			<li><a href="ramdas13-supp.pdf">Supplementary (PDF)</a></li>
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
